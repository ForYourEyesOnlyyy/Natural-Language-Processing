{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed17f4ed",
   "metadata": {
    "papermill": {
     "duration": 0.005313,
     "end_time": "2025-02-05T10:34:31.798201",
     "exception": false,
     "start_time": "2025-02-05T10:34:31.792888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Competition of **GloVe**s\n",
    "\n",
    "[Original paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "[Competition](https://www.kaggle.com/t/b32583e2b6054d6f90f16c9f638ce84d)\n",
    "\n",
    "GloVe is based on the idea that the learnt embedding should keep the fraction:\n",
    "\n",
    "\\begin{equation}\n",
    "F\\left(w_{i}, w_{j}, \\tilde{w}_{k}\\right)=\\frac{P_{i k}}{P_{j k}}\n",
    "\\end{equation}\n",
    "\n",
    "where i,j,k are indexes of some words, k - context, w - embedding from a model (same as Word2Vec). Probability P_{i k} is calculated from the corpus as  the probability that word i appear in the context of word k. Thus P_{i k} is determinant from the text and depend on context window lenght, while the embeddings are trainable.\n",
    "\n",
    "\n",
    "Applying the chain of assumptions about embeddings on the initial formula, the final relationship is:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{i}^{T} \\tilde{w}_{k}+b_{i}+\\tilde{b}_{k}=\\log \\left(X_{i k}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "which immediatly formulates as the least-squares objective:\n",
    "\n",
    "\\begin{equation}\n",
    "J=\\sum_{i, j=1}^{V} f\\left(X_{i j}\\right)\\left(w_{i}^{T} \\tilde{w}_{j}+b_{i}+\\tilde{b}_{j}-\\log X_{i j}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where V - vocabulary size, f - weight function of a token that bounds too frequent and too rare words:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x)=\\left\\{\\begin{array}{cc}\n",
    "\\left(x / x_{\\max }\\right)^\\alpha & \\text { if } x<x_{\\max } \\\\\n",
    "1 & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "wher $\\alpha$ and $x_{\\max}$ are hyperparameters. In the original experiments $\\alpha = 0.75$ and $x_{\\max} = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0765e4f",
   "metadata": {
    "papermill": {
     "duration": 0.004316,
     "end_time": "2025-02-05T10:34:31.807382",
     "exception": false,
     "start_time": "2025-02-05T10:34:31.803066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542e02c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:31.818228Z",
     "iopub.status.busy": "2025-02-05T10:34:31.817716Z",
     "iopub.status.idle": "2025-02-05T10:34:34.587433Z",
     "shell.execute_reply": "2025-02-05T10:34:34.586078Z"
    },
    "papermill": {
     "duration": 2.777224,
     "end_time": "2025-02-05T10:34:34.589252",
     "exception": false,
     "start_time": "2025-02-05T10:34:31.812028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /usr/share/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe42e48",
   "metadata": {
    "papermill": {
     "duration": 0.004819,
     "end_time": "2025-02-05T10:34:34.599149",
     "exception": false,
     "start_time": "2025-02-05T10:34:34.594330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2 Finish the implementation of GloVe\n",
    "\n",
    "There are several missed parts of code:\n",
    "\n",
    "- implementation of weightening function f\n",
    "- calculation of derivatives for J loss function\n",
    "- implementation of J\n",
    "- implementation of AdaGrad\n",
    "\n",
    "**You're free to use other libraries and implementations (for example, on torch or tf), except using of pretrained models**\n",
    "\n",
    "A nice-looking pseudo-code of AdaGrad could [be found here](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e2a935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:34.610946Z",
     "iopub.status.busy": "2025-02-05T10:34:34.610350Z",
     "iopub.status.idle": "2025-02-05T10:34:34.619228Z",
     "shell.execute_reply": "2025-02-05T10:34:34.618015Z"
    },
    "papermill": {
     "duration": 0.01655,
     "end_time": "2025-02-05T10:34:34.621073",
     "exception": false,
     "start_time": "2025-02-05T10:34:34.604523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_co_occurence_matrix(tokens, processed_sents, token2int, window_size=200):\n",
    "    '''\n",
    "    Calculate co-occurence of words, i.e. number of cases when some token appears in context of another\n",
    "\n",
    "    @tokens - vocabulary \n",
    "    @processed_sents - sentences from brown corpus\n",
    "    @token2int - dictionary that maps words with their indeces\n",
    "    @window_size - the radius of context, #of words from left and right side of a pivot token\n",
    "    '''\n",
    "    \n",
    "    # Calculate all occurences for a token_ in every sentences\n",
    "    def get_co_occurences(token_):\n",
    "        co_occurences = []\n",
    "        for sent in processed_sents:\n",
    "            for idx in (np.array(sent)==token_).nonzero()[0]:\n",
    "                co_occurences.append(sent[max(0, idx-window_size):min(idx+window_size+1, len(sent))])\n",
    "\n",
    "        co_occurences = list(itertools.chain(*co_occurences))\n",
    "        co_occurence_idxs = list(map(lambda x: token2int[x], co_occurences))\n",
    "        co_occurence_dict = Counter(co_occurence_idxs)\n",
    "        co_occurence_dict = dict(sorted(co_occurence_dict.items()))\n",
    "        return co_occurence_dict\n",
    "\n",
    "    co_occurence_matrix = np.zeros(shape=(len(tokens), len(tokens)), dtype='int')\n",
    "    for token in tokens:\n",
    "        token_idx = token2int[token]\n",
    "        co_occurence_dict = get_co_occurences(token)\n",
    "        co_occurence_matrix[token_idx, list(co_occurence_dict.keys())] = list(co_occurence_dict.values())\n",
    "        \n",
    "    # Zeroing the co-occurence of tokens with themselves    \n",
    "    np.fill_diagonal(co_occurence_matrix, 0)    \n",
    "    return co_occurence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99dc008",
   "metadata": {
    "papermill": {
     "duration": 0.004497,
     "end_time": "2025-02-05T10:34:34.630590",
     "exception": false,
     "start_time": "2025-02-05T10:34:34.626093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Numba](https://numba.pydata.org) would be critical in this competition because of complexity of algorithm. This library accelarates Python code and does not require installing additional components, compilers or languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f951d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:34.641808Z",
     "iopub.status.busy": "2025-02-05T10:34:34.641322Z",
     "iopub.status.idle": "2025-02-05T10:34:41.228242Z",
     "shell.execute_reply": "2025-02-05T10:34:41.226637Z"
    },
    "papermill": {
     "duration": 6.595695,
     "end_time": "2025-02-05T10:34:41.231147",
     "exception": false,
     "start_time": "2025-02-05T10:34:34.635452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\r\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.1,>=1.22->numba) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1,>=1.22->numba) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.1,>=1.22->numba) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.1,>=1.22->numba) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1097077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:41.242944Z",
     "iopub.status.busy": "2025-02-05T10:34:41.242521Z",
     "iopub.status.idle": "2025-02-05T10:34:42.785205Z",
     "shell.execute_reply": "2025-02-05T10:34:42.783913Z"
    },
    "papermill": {
     "duration": 1.551008,
     "end_time": "2025-02-05T10:34:42.787411",
     "exception": false,
     "start_time": "2025-02-05T10:34:41.236403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, extending, types\n",
    "\n",
    "@njit\n",
    "def f(X_wc, X_max=100, alpha=0.75):\n",
    "    # Weight of a token in the loss function\n",
    "    # Write your code. Find the formula in the paper\n",
    "    # (f(x) = (x/X_max)^alpha if x < X_max, else 1, and 0 if x=0)\n",
    "    if X_wc == 0:\n",
    "        return 0.0\n",
    "    elif X_wc < X_max:\n",
    "        return (X_wc / X_max) ** alpha\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "@njit\n",
    "def loss_fn(weights, bias, co_occurence_matrix, n_tokens, X_max, alpha):\n",
    "    total_loss = 0\n",
    "    for idx_word in range(n_tokens):\n",
    "        for idx_context in range(n_tokens):\n",
    "            w_word = weights[idx_word]\n",
    "            w_context = weights[n_tokens+idx_context]\n",
    "            b_word = bias[idx_word]\n",
    "            b_context = bias[n_tokens+idx_context]\n",
    "            X_wc = co_occurence_matrix[idx_word, idx_context]\n",
    "            # Write your code. Implement the loss function\n",
    "            if X_wc > 0:\n",
    "                total_loss += f(X_wc, X_max, alpha) * (\n",
    "                    (np.dot(w_word, w_context) + b_word + b_context - np.log(X_wc)) ** 2\n",
    "                )\n",
    "    return total_loss\n",
    "\n",
    "@njit\n",
    "def gradient(weights, bias, co_occurence_matrix, n_tokens, embedding_size, X_max, alpha):\n",
    "    dw = np.zeros((2*n_tokens, embedding_size))\n",
    "    db = np.zeros(2*n_tokens)\n",
    "\n",
    "    # building word vectors\n",
    "    for idx_word in range(n_tokens):\n",
    "        w_word = weights[idx_word]\n",
    "        b_word = bias[idx_word]\n",
    "\n",
    "        for idx_context in range(n_tokens):\n",
    "            w_context = weights[n_tokens+idx_context]\n",
    "            b_context = bias[n_tokens+idx_context]\n",
    "            X_wc = co_occurence_matrix[idx_word, idx_context]\n",
    "            # Derivative over loss function with respect to w_word \n",
    "            if X_wc > 0:\n",
    "                error = (np.dot(w_word, w_context) + b_word + b_context - np.log(X_wc))\n",
    "                value = 2.0 * f(X_wc, X_max, alpha) * error\n",
    "                db[idx_word] += value\n",
    "                dw[idx_word] += value * w_context\n",
    "\n",
    "    # building context vectors\n",
    "    for idx_context in range(n_tokens):\n",
    "        w_context = weights[n_tokens + idx_context]\n",
    "        b_context = bias[n_tokens + idx_context]\n",
    "\n",
    "        for idx_word in range(n_tokens):\n",
    "            w_word = weights[idx_word]\n",
    "            b_word = bias[idx_word]\n",
    "            X_wc = co_occurence_matrix[idx_word, idx_context]\n",
    "            # Derivative over loss function with respect to w_context \n",
    "            if X_wc > 0:\n",
    "                error = (np.dot(w_word, w_context) + b_word + b_context - np.log(X_wc))\n",
    "                value = 2.0 * f(X_wc, X_max, alpha) * error\n",
    "                db[n_tokens + idx_context] += value\n",
    "                dw[n_tokens + idx_context] += value * w_word\n",
    "    return dw, db\n",
    "        \n",
    "@njit\n",
    "def adagrad(co_occurence_matrix_, n_epochs, lr, alpha):\n",
    "    # We keep weights of context in the same matrix for simplicity\n",
    "    # (Assuming n_tokens, embedding_size are known globally or declared above in code)\n",
    "    n_tokens = co_occurence_matrix_.shape[0]\n",
    "\n",
    "    weights = np.random.random((2 * n_tokens, embedding_size))\n",
    "    bias = np.random.random((2 * n_tokens,))\n",
    "\n",
    "    state_sum_weights = np.zeros(weights.shape)\n",
    "    state_sum_bias = np.zeros(bias.shape)    \n",
    "    # Write your code. Choose an appropriate value for maximum co-occurence \n",
    "    # so that too-frequent words do not dominate. (Often 100 is standard.)\n",
    "    X_max = 2\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        dw, db = gradient(weights, bias, co_occurence_matrix_, n_tokens, embedding_size, X_max, alpha)\n",
    "\n",
    "        # Write your code. Finish the implementation of adagrad\n",
    "        state_sum_weights += dw**2\n",
    "        state_sum_bias += db**2\n",
    "\n",
    "        weights -= lr * dw / np.sqrt(state_sum_weights + 1e-8)\n",
    "        bias   -= lr * db / np.sqrt(state_sum_bias + 1e-8)\n",
    "\n",
    "        loss = loss_fn(weights, bias, co_occurence_matrix_, n_tokens, X_max, alpha)\n",
    "        print(\"Epoch \", i, \"| Loss  \", loss)\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32236d7",
   "metadata": {
    "papermill": {
     "duration": 0.00462,
     "end_time": "2025-02-05T10:34:42.797444",
     "exception": false,
     "start_time": "2025-02-05T10:34:42.792824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a36599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:42.808929Z",
     "iopub.status.busy": "2025-02-05T10:34:42.808508Z",
     "iopub.status.idle": "2025-02-05T10:34:42.813542Z",
     "shell.execute_reply": "2025-02-05T10:34:42.812343Z"
    },
    "papermill": {
     "duration": 0.013094,
     "end_time": "2025-02-05T10:34:42.815478",
     "exception": false,
     "start_time": "2025-02-05T10:34:42.802384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Because of small amount of dataset, we have to use many epochs\n",
    "n_epochs = 10\n",
    "# number of sentences to consider. Please don't reduce it, otherwise some words from test set might dissapear\n",
    "n_sents = 350\n",
    "# token embedding size\n",
    "embedding_size = 5\n",
    "# learning rate\n",
    "lr = 0.7\n",
    "# GloVe weights parameter\n",
    "alpha = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722c1036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:42.826684Z",
     "iopub.status.busy": "2025-02-05T10:34:42.826313Z",
     "iopub.status.idle": "2025-02-05T10:34:42.905246Z",
     "shell.execute_reply": "2025-02-05T10:34:42.904151Z"
    },
    "papermill": {
     "duration": 0.086671,
     "end_time": "2025-02-05T10:34:42.907123",
     "exception": false,
     "start_time": "2025-02-05T10:34:42.820452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences..\n",
      "Number of Sentences: 350\n",
      "Number of Tokens: 1820\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "sents = brown.sents()[:n_sents]\n",
    "\n",
    "print('Processing sentences..')\n",
    "processed_sents = []\n",
    "for sent in sents:\n",
    "    # Convert to lowercase and keep only alphabetic tokens\n",
    "    cleaned_sent = [word.lower() for word in sent if word.isalpha() and word.lower() not in stopwords]\n",
    "    processed_sents.append(cleaned_sent)\n",
    "\n",
    "tokens = list(set(itertools.chain(*processed_sents)))   \n",
    "n_tokens = len(tokens)\n",
    "print(f'Number of Sentences: {len(sents)}') \n",
    "print(f'Number of Tokens: {n_tokens}')\n",
    "\n",
    "token2int = dict(zip(tokens, range(len(tokens))))\n",
    "int2token = {v: k for k, v in token2int.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a16138",
   "metadata": {
    "papermill": {
     "duration": 0.004822,
     "end_time": "2025-02-05T10:34:42.917094",
     "exception": false,
     "start_time": "2025-02-05T10:34:42.912272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f863b55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:42.928204Z",
     "iopub.status.busy": "2025-02-05T10:34:42.927786Z",
     "iopub.status.idle": "2025-02-05T10:34:54.483954Z",
     "shell.execute_reply": "2025-02-05T10:34:54.482576Z"
    },
    "papermill": {
     "duration": 11.564137,
     "end_time": "2025-02-05T10:34:54.486057",
     "exception": false,
     "start_time": "2025-02-05T10:34:42.921920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building co-occurence matrix..\n",
      "Co-occurence matrix shape: (1820, 1820)\n",
      "Training word vectors..\n",
      "Epoch  0 | Loss   13129.918563723835\n",
      "Epoch  1 | Loss   4553.670548879382\n",
      "Epoch  2 | Loss   2978.4609459361245\n",
      "Epoch  3 | Loss   2488.667690094923\n",
      "Epoch  4 | Loss   2278.3930183800903\n",
      "Epoch  5 | Loss   2158.6890727628465\n",
      "Epoch  6 | Loss   2075.539493046873\n",
      "Epoch  7 | Loss   2010.2465488013358\n",
      "Epoch  8 | Loss   1955.304914085816\n",
      "Epoch  9 | Loss   1907.0985482966205\n"
     ]
    }
   ],
   "source": [
    "print('Building co-occurence matrix..')\n",
    "co_occurence_matrix = get_co_occurence_matrix(tokens, processed_sents, token2int)\n",
    "print('Co-occurence matrix shape:', co_occurence_matrix.shape)\n",
    "assert co_occurence_matrix.shape == (n_tokens, n_tokens)\n",
    "\n",
    "# co-occurence matrix is similar\n",
    "assert np.all(co_occurence_matrix.T == co_occurence_matrix)\n",
    "\n",
    "print('Training word vectors..')\n",
    "\n",
    "weights, bias = adagrad(co_occurence_matrix, n_epochs, lr, alpha)\n",
    "# Optional save\n",
    "# np.save('weights.npy', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077aa96",
   "metadata": {
    "papermill": {
     "duration": 0.005843,
     "end_time": "2025-02-05T10:34:54.497696",
     "exception": false,
     "start_time": "2025-02-05T10:34:54.491853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make sure that model returns adequate similar words. If it doesn't, maybe you should increase the corpus or(and) number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a41c580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:54.509958Z",
     "iopub.status.busy": "2025-02-05T10:34:54.509545Z",
     "iopub.status.idle": "2025-02-05T10:34:54.550551Z",
     "shell.execute_reply": "2025-02-05T10:34:54.549381Z"
    },
    "papermill": {
     "duration": 0.049313,
     "end_time": "2025-02-05T10:34:54.552411",
     "exception": false,
     "start_time": "2025-02-05T10:34:54.503098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to learn: ['charge', 'saba', 'impossible', 'ayes', 'request']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_words(csim, token):\n",
    "    token_idx = token2int[token]\n",
    "    closest_words = list(map(lambda x: int2token[x], np.argsort(csim[token_idx])[::-1][:5]))\n",
    "    return closest_words\n",
    "\n",
    "# getting cosine similarities between all combinations of word vectors\n",
    "csim = cosine_similarity(weights[:n_tokens])\n",
    "\n",
    "# masking diagonal values since they will be most similar\n",
    "np.fill_diagonal(csim, 0)\n",
    "\n",
    "token = 'learn'\n",
    "closest_words = find_similar_words(csim, token)\n",
    "print(f'Similar words to {token}:', closest_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e055285",
   "metadata": {
    "papermill": {
     "duration": 0.005034,
     "end_time": "2025-02-05T10:34:54.563023",
     "exception": false,
     "start_time": "2025-02-05T10:34:54.557989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.4 Calculate the  embeddings for given pairs of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88be3183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:54.575266Z",
     "iopub.status.busy": "2025-02-05T10:34:54.574885Z",
     "iopub.status.idle": "2025-02-05T10:34:55.578166Z",
     "shell.execute_reply": "2025-02-05T10:34:55.576620Z"
    },
    "papermill": {
     "duration": 1.011975,
     "end_time": "2025-02-05T10:34:55.580394",
     "exception": false,
     "start_time": "2025-02-05T10:34:54.568419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"/kaggle/input/nlp-week-2-glo-ve/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd24ef4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:55.592967Z",
     "iopub.status.busy": "2025-02-05T10:34:55.592579Z",
     "iopub.status.idle": "2025-02-05T10:34:55.612949Z",
     "shell.execute_reply": "2025-02-05T10:34:55.611920Z"
    },
    "papermill": {
     "duration": 0.028515,
     "end_time": "2025-02-05T10:34:55.614722",
     "exception": false,
     "start_time": "2025-02-05T10:34:55.586207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Similarity between pairs of synonyms \n",
    "cos_sims = {}\n",
    "for ind, row in df.iterrows():\n",
    "    word1 = row[\"word 1\"]\n",
    "    word2 = row[\"word 2\"]\n",
    "\n",
    "    # Try to process out-of-vocab words\n",
    "    try:\n",
    "        token_idx_1 = token2int[word1]\n",
    "        word_1_weight = np.expand_dims(weights[token_idx_1], axis=0)\n",
    "    except:\n",
    "        print(f\"Word {word1} is not in the vocabilary\")\n",
    "        word_1_weight = np.zeros((1, embedding_size))\n",
    "\n",
    "    try:\n",
    "        token_idx_2 = token2int[word2]\n",
    "        word_2_weight = np.expand_dims(weights[token_idx_2], axis=0)\n",
    "    except:\n",
    "        print(f\"Word {word2} is not in the vocabilary\")\n",
    "        word_2_weight = np.zeros((1, embedding_size))\n",
    "\n",
    "    csim = cosine_similarity(word_1_weight, word_2_weight)\n",
    "    cos_sims[ind] = csim[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5680d74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:55.626922Z",
     "iopub.status.busy": "2025-02-05T10:34:55.626562Z",
     "iopub.status.idle": "2025-02-05T10:34:55.639622Z",
     "shell.execute_reply": "2025-02-05T10:34:55.638330Z"
    },
    "papermill": {
     "duration": 0.021606,
     "end_time": "2025-02-05T10:34:55.641749",
     "exception": false,
     "start_time": "2025-02-05T10:34:55.620143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submision = pd.DataFrame(cos_sims.items(), columns=[\"ID\", \"sims\"])\n",
    "df_submision.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9f3969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:55.654029Z",
     "iopub.status.busy": "2025-02-05T10:34:55.653634Z",
     "iopub.status.idle": "2025-02-05T10:34:55.662270Z",
     "shell.execute_reply": "2025-02-05T10:34:55.661234Z"
    },
    "papermill": {
     "duration": 0.016874,
     "end_time": "2025-02-05T10:34:55.664178",
     "exception": false,
     "start_time": "2025-02-05T10:34:55.647304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.576218579207498"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submision['sims'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6297d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:34:55.677182Z",
     "iopub.status.busy": "2025-02-05T10:34:55.676764Z",
     "iopub.status.idle": "2025-02-05T10:34:55.682743Z",
     "shell.execute_reply": "2025-02-05T10:34:55.681683Z"
    },
    "papermill": {
     "duration": 0.014514,
     "end_time": "2025-02-05T10:34:55.684368",
     "exception": false,
     "start_time": "2025-02-05T10:34:55.669854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30602015190867704"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submision['sims'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98759652",
   "metadata": {
    "papermill": {
     "duration": 0.00527,
     "end_time": "2025-02-05T10:34:55.695611",
     "exception": false,
     "start_time": "2025-02-05T10:34:55.690341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10954574,
     "sourceId": 92051,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.668962,
   "end_time": "2025-02-05T10:34:56.827783",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-05T10:34:28.158821",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
